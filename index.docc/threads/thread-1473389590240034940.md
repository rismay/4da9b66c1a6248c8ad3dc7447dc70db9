# ðŸ§µ Token usage maximization (throttling / pulse model)

@Metadata {
  @PageKind(article)
  @PageColor(purple)
  @CallToAction(url: "https://discord.com/channels/1407786883773104278/1473389590240034940", label: "Open in Discord")
}

Thread: `1473389590240034940`

## Summary

This thread documents a token-usage / throughput strategy for running at max velocity under constraints (ex: a ~5 hour Codex window), including a named concept:

- **Throttling (pulse model / micro-bursting / pulse injection):** continuously allocate a baseline token budget plus frequent micro-injections (micro-bursts) to keep utilization near the ceiling while minimizing limiter penalties/violations and latency spikes.

## Key messages captured

- Spend tokens where they compound: specs/checklists first; diff-based work; batching.
- Starve the context window: maintain a small working set; move durable knowledge into memory/journal; start new sessions when threads get long.
- Use the right model for the job; measure/enforce budgets; default behaviors for ambiguity.

## Source

- Discord: https://discord.com/channels/1407786883773104278/1473389590240034940
